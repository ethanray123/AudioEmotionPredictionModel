{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of files: 764\n",
      "Length of Vocabulary 12060\n",
      "# of Tokens: 12060\n",
      "# of Lyrics: 764\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "    \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    filenames = listdir(directory)\n",
    "    filenames.sort()\n",
    "    print(\"Total # of files:\",len(filenames))\n",
    "    for filename in filenames:\n",
    "        # skip any reviews in the test set\n",
    "#         if filename.startswith('cv9'):\n",
    "#             continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "        # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# load all docs in a directory and place data in a list\n",
    "def read_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    filenames = listdir(directory)\n",
    "    filenames.sort()\n",
    "    # walk through all files in the folder\n",
    "    for filename in filenames:\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('MIREX-like_mood/dataset/Lyrics', vocab)\n",
    "# print the size of the vocab\n",
    "print(\"Length of Vocabulary\", len(vocab))\n",
    "# print the top words in the vocab\n",
    "# print(vocab.most_common(50))\n",
    "# keep tokens with a min occurrence\n",
    "min_occurance = 2\n",
    "# tokens = [k for k, c in vocab.items() if c >= min_occurance]\n",
    "tokens = [k for k, c in vocab.items()]\n",
    "print(\"# of Tokens:\",len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocabulary.txt')\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabulary.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "lines = np.array(read_docs('MIREX-like_mood/dataset/Lyrics/', vocab))\n",
    "# summarize what we have\n",
    "print(\"# of Lyrics:\",len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13915, 3)\n"
     ]
    }
   ],
   "source": [
    "ifile = open('Ratings_Warriner_et_al.csv', 'r')\n",
    "reader = csv.reader(ifile)\n",
    "words = []\n",
    "valence = []\n",
    "arousal = []\n",
    "for row in reader:\n",
    "    if(row[0] != \"\"):\n",
    "        words.append(row[1])\n",
    "        valence.append(row[2])\n",
    "        arousal.append(row[5])\n",
    "data = [words[0],valence[0],arousal[0]]\n",
    "for i in range(1,len(words)):\n",
    "    data_row = [words[i],valence[i],arousal[i]]\n",
    "    data = np.vstack((data, data_row))\n",
    "print(data.shape)\n",
    "\n",
    "np.save('rated_words.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aardvark\n",
      "ABOVE\n"
     ]
    }
   ],
   "source": [
    "wva = np.load('rated_words.npy')\n",
    "print(wva[0][0])\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocabulary.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab.sort()\n",
    "print(vocab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3870 words out of 12060\n"
     ]
    }
   ],
   "source": [
    "rated_words = wva[:,0]\n",
    "found_words = 0\n",
    "for word in vocab:\n",
    "    if(word in rated_words):\n",
    "        found_words+=1\n",
    "#         print(\"{} is in the rated words\".format(word))\n",
    "\n",
    "print(\"Found {} words out of {}\".format(found_words, len(vocab)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
